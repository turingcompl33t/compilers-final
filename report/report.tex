% Optimizing Compilers Final Project
% Final Report
% Abi Kim, Kyle Dotterrer, Wan Shen Lim
% 5 May, 2021

%% ==================================================================
%% BOILERPLATE
%% ==================================================================

\documentclass{vldb}

\newcommand{\paperTitle}{Profile Generation and Guided Optimization\\ for Compiled Queries\\ 
    \large Final Report\\
    \today}
\newcommand{\paperKeywords}{}
\newcommand{\paperAuthors}{}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\usepackage[
            bookmarks=true,
            bookmarksopen=true,
            pdfhighlight=/I,
            pdfpagemode=UseOutlines,
            linkcolor=blue,
            pdfborder={ 0 0 0 },
            pageanchor=false]{hyperref}
\hypersetup{
    pdfauthor = {\paperAuthors},
    pdftitle = {\paperTitle},
    pdfkeywords = {\paperKeywords},
    pdfborder={ 0 0 0 }
}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{times}
\usepackage{boxedminipage}
\usepackage{xspace}
\usepackage{array}
\usepackage{epsfig}
\usepackage{calc}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage[usenames,dvipsnames]{color}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{wasysym }
\usepackage[hyphenbreaks]{breakurl}
\usepackage{xcolor}
\usepackage[font={small}]{caption}
\usepackage{alltt}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{etoolbox}

% Font selection
\usepackage[final]{microtype}
\usepackage[scaled]{inconsolata}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{pifont}

% Other misc. things
\usepackage{float}
\usepackage{bookmark}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{emptypage}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{braket}
\usepackage{balance}
\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}

%% ==================================================================
%% MACROS
%% ==================================================================

\newcommand{\subparagraph}{}

\newcommand{\sysname}{NoisePage\xspace}

% Framework Components
\newcommand{\dbTranslator}{Translator\xspace}
\newcommand{\dbCompiler}{Compiler\xspace}
\newcommand{\dbPlanner}{planner\xspace}
\newcommand{\dbInterpreter}{interpreter\xspace}
\newcommand{\dbAnalyzer}{analyzer\xspace}
\newcommand{\dbPCQ}{PCQ\xspace}
\newcommand{\dbAQP}{AQP\xspace}
\newcommand{\dbHook}{hook\xspace}
\newcommand{\dbHooks}{hooks\xspace}

\newcommand{\dbSQL}[1]{\texttt{\textbf{#1}}\xspace}
\newcommand{\dbTable}[1]{\texttt{\MakeUppercase{#1}}\xspace}
\newcommand{\dbColumn}[1]{\texttt{col#1}\xspace}
\newcommand{\dbRawColumn}[1]{\texttt{#1}\xspace}
\newcommand{\dbConfig}[1]{\textsf{#1}\xspace}

\newcommand{\dbmsOracle}{Oracle\xspace}
\newcommand{\dbmsDBTwo}{DB2\xspace}
\newcommand{\dbmsTeradata}{Teradata\xspace}
\newcommand{\dbmsHANA}{HANA\xspace}
\newcommand{\dbmsMSSQL}{SQL Server\xspace}
\newcommand{\dbmsMySQL}{MySQL\xspace}
\newcommand{\dbmsPostgres}{PostgreSQL\xspace}
\newcommand{\dbmsMemSQL}{MemSQL\xspace}
\newcommand{\dbmsHyper}{HyPer\xspace}
\newcommand{\dbmsVector}{Vector\xspace}

\newcommand{\tplType}[1]{\texttt{#1}\xspace}
\newcommand{\tpl}{\texttt{TPL}\xspace}
\newcommand{\tbc}{\texttt{TBC}\xspace}
\newcommand{\dbCode}[1]{{\sffamily\small \textbf{#1}}\xspace}

\newcommand{\tidlist}{\texttt{TupleIdList}\xspace}
\newcommand{\tidlists}{\texttt{TupleIdLists}\xspace}

%% ==================================================================
%% PAPER CONTENT
%% ==================================================================

\begin{document}

\title{\paperTitle}

\numberofauthors{3}
\author{
    \alignauthor Abi Kim, Kyle Dotterrer, Wan Shen Lim\\
    \affaddr{Carnegie Mellon University}\\
}

% Remove the space for the copyright box for VLDB
\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\maketitle

%% ==================================================================
%% INTRODUCTION
%% ==================================================================

\section{Introduction}

This document describes our milestone report for our final project in CMU's 15-745: \textit{Optimizing Compilers} course. 

\subsection{Problem Statement}

The goal of our project is to improve the runtime performance of query execution in a data-centric code generation DBMS engine by integrating dynamic optimization techniques. Specifically, we aim to implement profile-guided optimization for JIT-compiled queries in the NoisePage \cite{noisepage} LLVM-based execution engine. We partition this problem into the following sub-problems:

\begin{itemize}
    \item Implement support for collection of low-level statistics and profile information in the NoisePage JIT. These statistics may include runtime information for individual operators at arbitrary abstraction levels that appear during the lowering process within the NoisePage execution engine. Examples of supported operators might include SQL operators, operator pipelines, and TPL functions.  Adding support for collection of these statistics enables the implementation of SQL’s \dbCode{EXPLAIN ANALYZE} command. More importantly, the profile information collected with this mechanism may be used as input to other components of the system.
    \item Implement a feedback loop mechanism that allows for an automated feedback-driven exploration of the space of potential LLVM optimization passes, based on the profile information generated by prior iterations.
    \item Explore techniques for efficient instrumentation of fused operators. Sufficient gains in efficiency might allow us to gather and utilize profile information in an online setting, rather than merely across queries.
\end{itemize}

In addressing these problems, salient challenges arise from the following sources:

\begin{itemize}
    \item \textbf{Query Compilation}: Collecting query execution profiles in database systems that execute queries in software can be accomplished via traditional software profiling techniques. These techniques break down for systems like NoisePage that compile queries to native code and execute them directly on hardware because the execution profile query is completely distinct from the execution profile for the system that compiled it.
    \item \textbf{Operator Fusion}: Execution profile collection is further complicated by the operator fusion optimization technique employed by the NoisePage execution engine [ROF]. In a less sophisticated execution engine, each operator in the physical plan tree for the query might be translated directly to a function in the code generated for the query. However, NoisePage employs the operator fusion technique in which multiple operators in the physical query plan may be fused into a single pipeline abstraction that is subsequently compiled to a single function in the generated code. While it improves query runtime by limiting temporary data materialization, operator fusion destroys the relationship between the generated code and the higher-level abstractions in the system.
    \item \textbf{Optimization Pass(es)}: We need to determine the appropriate pass or passes to implement that will be able to make effective use of the profile data we intend to gather. We assume that only a subset of available profile-guided optimization passes will be applicable in the context of our project.
    \item \textbf{The DBMS-Centric Context}: Some additional considerations introduced by the DBMS setting are:
    \begin{itemize}
        \item Highly multi-user: no single query should starve the entire system of resources.
        \item Time to first response matters: it is not acceptable to spend too long optimizing instead of quickly responding.
        \item Adaptive execution allows more time to be spent compiling anyway: fast OLTP queries are run through an interpreter, long OLAP queries are compiled in the background and swapped in when compilation is complete. This model lends itself well to multiple stages of compilation with different optimization levels. We are unaware of existing projects that draw a link between adaptive execution and profile-guided optimization.
    \end{itemize}
\end{itemize}

\subsection{Approach}

We partition our approach in this project into the following two thrusts:

\textbf{Thrust \#1}: Implement support for execution profile generation in the NoisePage execution engine. Near the midpoint of the project timeline, the TUM group published a paper at EuroSys 2021 that presents a general solution to the problem of profile generation across abstraction levels in dataflow systems \cite{beischl21}. The tailored profiling technique introduced in this work is highly applicable to the implementation of our profiling subsystem and provides a roadmap for successfully solving one of the primary problems we address in this project. We adopt the implementation of tailored profiling in  our implementation of a profiling subsystem within NoisePage. 

\textbf{Thrust \#2}: Implement an automated feedback-driven exploration of the space of potential LLVM optimization passes, applied at whole-program or individual-function granularity.

\subsection{Related Work}

The recent EuroSys 2021 publication is both timely and highly-relevant to the implementation of our profiling subsystem \cite{beischl21}. This paper describes a general solution to the problem of profiling dataflow systems at an abstraction level that is appropriate for the problem domain. Furthermore, this paper also describes integration of tailored profiling with the data-centric code generation engine within the Umbra \cite{umbra} DBMS, making it a true model for our system. Prior works in this area present solutions that are applicable within limited domains \cite{stuart20} or require substantial manual effort during post-processing \cite{noll20}.

There are separate branches of work that relate to optimizing queries for a code generation DBMS engine. Examples of code generation DBMS engines include HyPer \cite{hyper} and SingleStore \cite{singlestore}, but typically all of the program's code is compiled at the same optimization level with the same flags. Projects that varied compilers and compiler flags to find the best possible code for a primitive have been explored in the context of the Vectorwise \cite{raducanu12} system. However, the implementation in Vectorwise relies on a-priori knowledge of the primitives that will be compiled. This approach is therefore less flexible than the implementation within NoisePage which can generate arbitrary functions at runtime without any a-priori information.

\subsection{Contributions}

In this project, we make the following contributions:

\begin{itemize}
    \item Explore the feasibility of implementing a profiling subsystem via \textit{tailored profiling} \cite{beischl21} in the NoisePage DBMS. A key insight is that often, an optimizing compiler must make decisions based on limited or even no information as to how its optimizations will affect the overall program’s efficiency. By implementing a profiling subsystem that can track profile data across multiple abstraction levels, a profile-guided optimizer will have a much richer set of information to understand the budget, scope, and impact of any optimizations made. 
    \item Explore the feasibility of automated search strategies for determining what set of LLVM transform passes to apply and at what granularity, e.g., whole-program vs function-level. A key insight is that 
    \item Evaluate our strategy against a set of hand-picked globally applied LLVM function transform passes that were empirically chosen by a DBMS+compilers domain expert Dr. Menon. While we find that Dr. Menon is very difficult to defeat on similar grounds (whole-program optimization), we find that our strategy is effective at rapidly sampling the space of potential optimizations to find promising and cheap transformations for a given SQL query, which allows for a number of exploratory techniques such as iterating on the domain expert's knowledge with genetic algorithms, completely random sampling of complete pipelines in hopes of lottery tickets, etc.
\end{itemize}

The remainder of this document is structured as follows. First, in Section 2, we provide a detailed description of our design and approach. Next, in Sections 3 and 4, we describe our experimental setup and the results of our evaluation. Section 5 discusses some surprised and lessons learned over the course of the project. We conclude in Section 6 and comment on potential directions for future work.

%% ==================================================================
%% DESIGN AND APPROACH
%% ==================================================================

\section{Design and Approach}

\subsection{Profiling across Abstraction Boundaries}

In order to provide the automated optimization exploration framework with richer information than just total execution time, we implement a profiling subsystem within NoisePage. This subsystem allows us to collect a fine-grained execution profile for JIT-compiled query code.

\subsubsection{Implementation False Starts}

We made two initial attempts at implementing the profiling subsystem that ultimately proved to be false starts:

\begin{itemize}
    \item \textbf{Attempt \#1: Solve it in C++} We attempted to modify the C++ translators that convert DBMS physical plans to \tpl code by inserting calls to instrumentation functions. This proved tedious, unwieldy, error-prone, and provided only low-resolution information. 
    \item \textbf{Attempt \#2: Solve it in \tpl} We have (and continue to use) implemented the recording of information at execution time into DSL-level structs. This makes it convenient to record features such as the number of entries into a given function. However, this requires all the information that you want to record to be exposed in the DSL level itself, which may not be feasible and is a lot of tedium.
\end{itemize}

We abandoned both of these approaches upon learning of the general solution to profiling across abstraction levels presented by Beischl et al. \cite{beischl21}.

\subsubsection{Final Design Overview}

The final design of our profiling subsystem is based on the tailored profiling design described by the Beischl et al. \cite{beischl21}.

The profiling subsystem assumes responsibility for two distinct tasks:

\begin{itemize}
    \item \textbf{Tagging Dictionary Construction}: The tagging dictionary is a multi-level dictionary data structure that allows us to effectively undo the lowering performed by the NoisePage execution engine during the code generation and query compilation process. Each level of the tagging dictionary maps individual operations in some intermediate representation to the operation at the next higher level of abstraction that produced it. The tagging dictionary is constructed as the query is lowered from the physical query plan tree to native machine code.
    \item \textbf{Runtime Sample Collection}: We utilize the Linux perf API \cite{perf} to collect an execution profile for JIT-compiled queries as they are executed. 
\end{itemize}

\subsubsection{Tagging Dictionary Construction}

Our implementation of the tagging dictionary recognizes the following five intermediate representations (from highest to lowest abstraction level):
\begin{itemize}
    \item SQL Operator
    \item \tpl AST Node
    \item \tpl Bytecode
    \item LLVM IR
    \item x86 Machine Code
\end{itemize}

We utilize a tagging dictionary with three levels to record the translations that are performed across the entirety of the lowering process. The lowest level of translation from x86 to LLVM IR is accomplished via debugging information emitted by the LLVM JIT when the native code object is originally produced. A brief description of the implementation required to populate each level of the tagging dictionary is provided below.
\begin{itemize}
    \item \textbf{\tpl AST Node to SQL Operator}: We populate this level of the tagging dictionary during the first phase of code generation. We modify the code generator’s operator translators to emit tags whenever a new AST node is constructed.
    \item \textbf{\tpl Bytecode to \tpl AST Node}: We populate this level of the tagging dictionary during the second phase of code generation. We add functionality to the TPL compiler that allows us to inject a callback function that emits a tag whenever the bytecode generator emits a new bytecode instruction.
    \item \textbf{LLVM IR Instruction to \tpl Bytecode}: We populate this level of the tagging dictionary during query compilation. We manually insert calls to emit tags whenever a new IR instruction is inserted into the compiled module.
\end{itemize}

\subsubsection{Profile Sample Collection}

We utilize the Linux perf API to collect an execution profile for the JIT-compiled query as it is executed. We utilize event-based sampling on instruction retirement (\dbCode{INST\_RETIRED.PREC\_DIST}) events. A sample is collected every 5,000 events. To ensure the accuracy of the collected samples, we request precise instruction pointers (0 skid) in our samples; this enables the Processor Event Based Sampling (PEBS) feature on Intel platforms \cite{intelmanual}. 

Samples are written to a memory-mapped ring buffer by the kernel at the specified interval. We read the raw samples from this ring buffer and serialize them to a JSON file for subsequent processing.

\subsubsection{Tailored Profiling}

We combine the information collected in the tagging dictionary with the samples collected during query execution to generate profiles for any intermediate representation within NoisePage. For example, tailored profiling enables implementation of the SQL \dbCode{EXPLAIN ANALYZE} command by mapping native machine instructions all the way back up to SQL operators. However, it might also be useful to analyze profiles for lower levels of abstraction such as \tpl functions. Our implementation of tailored profiling is flexible enough to support arbitrary post-processing of this kind.

\subsection{Programmatic Pass Application}

We are unable to use LLVM's new non-legacy function pass manager for some annoying build-related reasons. By simply assigning each pass an ID, (in fact just the index of the pass in the overall array of possible transforms), we are able to naively separate the design of our automated pass exploration into two components:

\subsubsection{Component \#1: Algorithms on Lists of Natural Numbers}

A set of transformation passes is conceptually represented as a list of natural numbers, possibly with repeats. Each number in the list maps to a specified LLVM transform pass. For example, [3,10,2,10] may correspond to the application of the LLVM transform passses [gvn, simplifycfg, adce, simplifycfg]. This representation makes enumerating the search space convenient:

\begin{lstlisting}
# Pick up to max_passes elements from passes.
def uniform_selection(passes, max_passes):
return [random(0, len(passes)) for _ in range(0, max_passes)]

# Mutate a random pass in the list.
def mutate(passes):
    passes[random(0, len(passes))] = random(0, len(passes))

# Either mutate, add, or delete a pass in the list. This is a modification of a genetic/evolutionary algorithm.
def genetic(passes):
    choice = random(1, 4)
    if x==1: passes[random(0,len(passes))]=random(0, len(passes))
    else if x == 2: passes.append(random(0, max_passes))
    else if x == 3: passes.erase(random(0, len(passes))
    # x == 4 is no-op
    
# Pick a random ordering of distinct passes such that each pass improves the time frame significantly.
def distinct_ordering(passes):
    passes.append(random(0,max_passes) s.t. passes are distinct)
\end{lstlisting}

Any typical search algorithms such as beam search, genetic evolution, etc. would be implementable in this framework. However, due to time constraints, our results (below) were largely obtained with simple genetic search.

\subsubsection{Component \#2: A Framework for Applying an Algorithm to Select Passes}

A typical code generation DBMS engine requires some finagling to get the execution time results reported back into the compilation pipeline. The issue is that compile-and-optimize level statistics will happen at a different time and call site than execution-time statistics. The combination of the above profiling subsystem and our framework for pass application is ongoing and is likely to extend beyond this course; the current state of the framework is only able to gather execution-time statistics for functions which are manually invoked. To better understand the last limitation, consider the following example of a typical \tpl plan for inserting into a table, which may have many functions:

\begin{lstlisting}
fun Setup() { ... }
fun Body() { 
    ...;
    insertIntoTable(&tableIterator, &row);
    ...;
}
fun Teardown() { ... }
\end{lstlisting}

Where the compiled \tpl program is invoked by explicitly executing \dbCode{Setup()}, \dbCode{Body()}, and \dbCode{Teardown()} in that order (modulo aborts or other exceptions). We can gather execution times for explicitly executed functions like \dbCode{Setup()}, \dbCode{Body()}, and \dbCode{Teardown()}, but we cannot gather times for the \dbCode{insertIntoTable()} function. Because the majority of our heuristics are currently focused on execution time, this is a significant and implementation-specific handicap to our approach, especially when we see a surprising amount of gain from optimizing these library functions instead of what is considered our “main” TPL program. Please see a discussion of the results below.

\subsubsection{Discussion}

We would like to highlight the flexibility of this two-component approach in allowing feature-engineered passes to be treated the same way as regular LLVM passes, with an example shown below.

\begin{lstlisting}
{"pmenon",
[](llvm::legacy::FunctionPassManager &fpm) {
    // Add custom passes. Hand-selected based on empirical evaluation.
    fpm.add(llvm::createInstructionCombiningPass());
    fpm.add(llvm::createReassociatePass());
    fpm.add(llvm::createGVNPass());
    fpm.add(llvm::createCFGSimplificationPass());
    fpm.add(llvm::createAggressiveDCEPass());
    fpm.add(llvm::createCFGSimplificationPass());
}}, 
\end{lstlisting}

Additionally, in the DBMS context, care has to be taken to perform all profiling work in a "fake transaction", which is our phrase for a transaction that always gets aborted. This proved to be slightly tricky due to certain implementation details of how results were transmitted back from our JIT'ed code back into the network layer -- the code would normally write results back to the Postgres-compatible packet writer as they became available, for example. In this case, we benefit significantly from the concept of aborts being routine in a DBMS setting because it allows us to execute code that has side effects while essentially ignoring the side effects. It would be interesting to study how side effects are handled in other systems which attempt profile-guided optimization.

To track whether an improvement was truly made, we always use the minimum -- noise is an issue, but noise cannot make slow code go faster, and over the many (hundreds/thousands) of iterations of our function pass applications, the minimum should win out. The algorithms that are used must ensure that mistakes can be recovered from, e.g., by allowing for arbitrary pass addition, mutation, or deletion.

%% ==================================================================
%% EXPERIMENTAL SETUP
%% ==================================================================

\section{Experimental Setup}

%% ==================================================================
%% EXPERIMENTAL EVALUATION
%% ==================================================================

\section{Experimental Evaluation}

%% ==================================================================
%% SURPRISES AND LESSONS LEARNED
%% ==================================================================

\section{Surprises and Lessons Learned}

%% ==================================================================
%% CONCLUSION AND FUTURE WORK
%% ==================================================================

\section{Conclusion and Future Work}

%% ==================================================================
%% WORK DISTRIBUTION
%% ==================================================================

\section{Work Distribution}

All members of our group contributed equally to this project.

%% ==================================================================
%% BIBLIOGRAPHY
%% ==================================================================

% NOTE: Had to add this mysterious command to compile...
\newcommand{\newblock}{}

\newpage
\balance
\bibliographystyle{abbrv}
\bibliography{report}

\end{document}