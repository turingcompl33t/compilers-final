% Optimizing Compilers Final Project
% Final Report
% Abi Kim, Kyle Dotterrer, Wan Shen Lim
% 5 May, 2021

%% ==================================================================
%% BOILERPLATE
%% ==================================================================

\documentclass{vldb}

\newcommand{\paperTitle}{Profile Generation and Guided Optimization\\ for Compiled Queries\\ 
    \large Final Report\\
    \today}
\newcommand{\paperKeywords}{}
\newcommand{\paperAuthors}{}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\usepackage[
            bookmarks=true,
            bookmarksopen=true,
            pdfhighlight=/I,
            pdfpagemode=UseOutlines,
            linkcolor=blue,
            pdfborder={ 0 0 0 },
            pageanchor=false]{hyperref}
\hypersetup{
    pdfauthor = {\paperAuthors},
    pdftitle = {\paperTitle},
    pdfkeywords = {\paperKeywords},
    pdfborder={ 0 0 0 }
}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{times}
\usepackage{boxedminipage}
\usepackage{xspace}
\usepackage{array}
\usepackage{epsfig}
\usepackage{calc}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage[usenames,dvipsnames]{color}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{wasysym }
\usepackage[hyphenbreaks]{breakurl}
\usepackage{xcolor}
\usepackage[font={small}]{caption}
\usepackage{alltt}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{etoolbox}

% Font selection
\usepackage[final]{microtype}
\usepackage[scaled]{inconsolata}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{pifont}

% Other misc. things
\usepackage{float}
\usepackage{bookmark}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{emptypage}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{braket}
\usepackage{balance}
\usepackage{listings}
\usepackage{enumitem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}

%% ==================================================================
%% MACROS
%% ==================================================================

\newcommand{\subparagraph}{}

\newcommand{\sysname}{NoisePage\xspace}

% Framework Components
\newcommand{\dbTranslator}{Translator\xspace}
\newcommand{\dbCompiler}{Compiler\xspace}
\newcommand{\dbPlanner}{planner\xspace}
\newcommand{\dbInterpreter}{interpreter\xspace}
\newcommand{\dbAnalyzer}{analyzer\xspace}
\newcommand{\dbPCQ}{PCQ\xspace}
\newcommand{\dbAQP}{AQP\xspace}
\newcommand{\dbHook}{hook\xspace}
\newcommand{\dbHooks}{hooks\xspace}

\newcommand{\dbSQL}[1]{\texttt{\textbf{#1}}\xspace}
\newcommand{\dbTable}[1]{\texttt{\MakeUppercase{#1}}\xspace}
\newcommand{\dbColumn}[1]{\texttt{col#1}\xspace}
\newcommand{\dbRawColumn}[1]{\texttt{#1}\xspace}
\newcommand{\dbConfig}[1]{\textsf{#1}\xspace}

\newcommand{\dbmsOracle}{Oracle\xspace}
\newcommand{\dbmsDBTwo}{DB2\xspace}
\newcommand{\dbmsTeradata}{Teradata\xspace}
\newcommand{\dbmsHANA}{HANA\xspace}
\newcommand{\dbmsMSSQL}{SQL Server\xspace}
\newcommand{\dbmsMySQL}{MySQL\xspace}
\newcommand{\dbmsPostgres}{PostgreSQL\xspace}
\newcommand{\dbmsMemSQL}{MemSQL\xspace}
\newcommand{\dbmsHyper}{HyPer\xspace}
\newcommand{\dbmsVector}{Vector\xspace}

\newcommand{\tplType}[1]{\texttt{#1}\xspace}
\newcommand{\tpl}{\texttt{TPL}\xspace}
\newcommand{\tbc}{\texttt{TBC}\xspace}
\newcommand{\dbCode}[1]{{\sffamily\small \textbf{#1}}\xspace}

\newcommand{\tidlist}{\texttt{TupleIdList}\xspace}
\newcommand{\tidlists}{\texttt{TupleIdLists}\xspace}

%% ==================================================================
%% PAPER CONTENT
%% ==================================================================

\begin{document}

\title{\paperTitle}

\numberofauthors{3}
\author{
    \alignauthor Abi Kim, Kyle Dotterrer, Wan Shen Lim\\
    \affaddr{Carnegie Mellon University}\\
}

% Remove the space for the copyright box for VLDB
\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\maketitle

%% ==================================================================
%% INTRODUCTION
%% ==================================================================

\section{Introduction}

This document describes our milestone report for our final project in CMU's 15-745: \textit{Optimizing Compilers} course. 

\subsection{Problem Statement}

The goal of our project is to improve the runtime performance of query execution in a data-centric code generation DBMS engine by integrating dynamic optimization techniques. Specifically, we aim to implement profile-guided optimization for JIT-compiled queries in the NoisePage \cite{noisepage} LLVM-based execution engine. We partition this problem into the following sub-problems:

\begin{itemize}
    \item Implement support for collection of low-level statistics and profile information in the NoisePage JIT. These statistics may include runtime information for individual operators at arbitrary abstraction levels that appear during the lowering process within the NoisePage execution engine. Examples of supported operators might include SQL operators, operator pipelines, and TPL functions.  Adding support for collection of these statistics enables the implementation of SQL's \dbCode{EXPLAIN ANALYZE} command. More importantly, the profile information collected with this mechanism may be used as input to other components of the system.
    \item Implement a feedback loop mechanism that allows for an automated feedback-driven exploration of the space of potential LLVM optimization passes, based on the profile information generated by prior iterations.
    \item Explore techniques for efficient instrumentation of fused operators. Sufficient gains in efficiency might allow us to gather and utilize profile information in an online setting, rather than merely across queries.
\end{itemize}

In addressing these problems, salient challenges arise from the following sources:

\begin{itemize}
    \item \textbf{Query Compilation}: Collecting query execution profiles in database systems that execute queries in software can be accomplished via traditional software profiling techniques. These techniques break down for systems like NoisePage that compile queries to native code and execute them directly on hardware because the execution profile query is completely distinct from the execution profile for the system that compiled it.
    \item \textbf{Operator Fusion}: Execution profile collection is further complicated by the operator fusion optimization technique employed by the NoisePage execution engine [ROF]. In a less sophisticated execution engine, each operator in the physical plan tree for the query might be translated directly to a function in the code generated for the query. However, NoisePage employs the operator fusion technique in which multiple operators in the physical query plan may be fused into a single pipeline abstraction that is subsequently compiled to a single function in the generated code. While it improves query runtime by limiting temporary data materialization, operator fusion destroys the relationship between the generated code and the higher-level abstractions in the system.
    \item \textbf{Optimization Pass(es)}: We need to determine the appropriate pass or passes to implement that will be able to make effective use of the profile data we intend to gather. We assume that only a subset of available profile-guided optimization passes will be applicable in the context of our project.
    \item \textbf{The DBMS-Centric Context}: Some additional considerations introduced by the DBMS setting are:
    \begin{itemize}
        \item Highly multi-user: no single query should starve the entire system of resources.
        \item Time to first response matters: it is not acceptable to spend too long optimizing instead of quickly responding.
        \item Adaptive execution allows more time to be spent compiling anyway: fast OLTP queries are run through an interpreter, long OLAP queries are compiled in the background and swapped in when compilation is complete. This model lends itself well to multiple stages of compilation with different optimization levels. We are unaware of existing projects that draw a link between adaptive execution and profile-guided optimization.
    \end{itemize}
\end{itemize}

\subsection{Approach}

We partition our approach in this project into the following two thrusts:

\textbf{Thrust \#1}: Implement support for execution profile generation in the NoisePage execution engine. Near the midpoint of the project timeline, the TUM group published a paper at EuroSys 2021 that presents a general solution to the problem of profile generation across abstraction levels in dataflow systems \cite{beischl21}. The tailored profiling technique introduced in this work is highly applicable to the implementation of our profiling subsystem and provides a roadmap for successfully solving one of the primary problems we address in this project. We adopt the implementation of tailored profiling in  our implementation of a profiling subsystem within NoisePage. 

\textbf{Thrust \#2}: Implement an automated feedback-driven exploration of the space of potential LLVM optimization passes, applied at whole-program or individual-function granularity.

\subsection{Related Work}

The recent EuroSys 2021 publication is both timely and highly-relevant to the implementation of our profiling subsystem \cite{beischl21}. This paper describes a general solution to the problem of profiling dataflow systems at an abstraction level that is appropriate for the problem domain. Furthermore, this paper also describes integration of tailored profiling with the data-centric code generation engine within the Umbra \cite{umbra} DBMS, making it a true model for our system. Prior works in this area present solutions that are applicable within limited domains \cite{stuart20} or require substantial manual effort during post-processing \cite{noll20}.

There are separate branches of work that relate to optimizing queries for a code generation DBMS engine. Examples of code generation DBMS engines include HyPer \cite{hyper} and SingleStore \cite{singlestore}, but typically all of the program's code is compiled at the same optimization level with the same flags. Projects that varied compilers and compiler flags to find the best possible code for a primitive have been explored in the context of the Vectorwise \cite{raducanu12} system. However, the implementation in Vectorwise relies on a-priori knowledge of the primitives that will be compiled. This approach is therefore less flexible than the implementation within NoisePage which can generate arbitrary functions at runtime without any a-priori information.

\subsection{Contributions}

In this project, we make the following contributions:

\begin{itemize}
    \item Explore the feasibility of implementing a profiling subsystem via \textit{tailored profiling} \cite{beischl21} in the NoisePage DBMS. A key insight is that often, an optimizing compiler must make decisions based on limited or even no information as to how its optimizations will affect the overall program’s efficiency. By implementing a profiling subsystem that can track profile data across multiple abstraction levels, a profile-guided optimizer will have a much richer set of information to understand the budget, scope, and impact of any optimizations made. 
    \item Explore the feasibility of automated search strategies for determining what set of LLVM transform passes to apply and at what granularity, e.g., whole-program vs function-level. A key insight is that 
    \item Evaluate our strategy against a set of hand-picked globally applied LLVM function transform passes that were empirically chosen by a DBMS+compilers domain expert Dr. Menon. While we find that Dr. Menon is very difficult to defeat on similar grounds (whole-program optimization), we find that our strategy is effective at rapidly sampling the space of potential optimizations to find promising and cheap transformations for a given SQL query, which allows for a number of exploratory techniques such as iterating on the domain expert's knowledge with genetic algorithms, completely random sampling of complete pipelines in hopes of lottery tickets, etc.
\end{itemize}

The remainder of this document is structured as follows. First, in Section 2, we provide a detailed description of our design and approach. Next, in Sections 3 and 4, we describe our experimental setup and the results of our evaluation. Section 5 discusses some surprised and lessons learned over the course of the project. We conclude in Section 6 and comment on potential directions for future work.

%% ==================================================================
%% DESIGN AND APPROACH
%% ==================================================================

\section{Design and Approach}

\subsection{Profiling across Abstraction Boundaries}

In order to provide the automated optimization exploration framework with richer information than just total execution time, we implement a profiling subsystem within NoisePage. This subsystem allows us to collect a fine-grained execution profile for JIT-compiled query code.

\subsubsection{Implementation False Starts}

We made two initial attempts at implementing the profiling subsystem that ultimately proved to be false starts:

\begin{itemize}
    \item \textbf{Attempt \#1: Solve it in C++} We attempted to modify the C++ translators that convert DBMS physical plans to \tpl code by inserting calls to instrumentation functions. This proved tedious, unwieldy, error-prone, and provided only low-resolution information. 
    \item \textbf{Attempt \#2: Solve it in \tpl} We have (and continue to use) implemented the recording of information at execution time into DSL-level structs. This makes it convenient to record features such as the number of entries into a given function. However, this requires all the information that you want to record to be exposed in the DSL level itself, which may not be feasible and is a lot of tedium.
\end{itemize}

We abandoned both of these approaches upon learning of the general solution to profiling across abstraction levels presented by Beischl et al. \cite{beischl21}.

\subsubsection{Final Design Overview}

The final design of our profiling subsystem is based on the tailored profiling design described by the Beischl et al. \cite{beischl21}.

The profiling subsystem assumes responsibility for two distinct tasks:

\begin{itemize}
    \item \textbf{Tagging Dictionary Construction}: The tagging dictionary is a multi-level dictionary data structure that allows us to effectively undo the lowering performed by the NoisePage execution engine during the code generation and query compilation process. Each level of the tagging dictionary maps individual operations in some intermediate representation to the operation at the next higher level of abstraction that produced it. The tagging dictionary is constructed as the query is lowered from the physical query plan tree to native machine code.
    \item \textbf{Runtime Sample Collection}: We utilize the Linux perf API \cite{perf} to collect an execution profile for JIT-compiled queries as they are executed. 
\end{itemize}

\subsubsection{Tagging Dictionary Construction}

Our implementation of the tagging dictionary recognizes the following five intermediate representations (from highest to lowest abstraction level):
\begin{itemize}
    \item SQL Operator
    \item \tpl AST Node
    \item \tpl Bytecode
    \item LLVM IR
    \item x86 Machine Code
\end{itemize}

We utilize a tagging dictionary with three levels to record the translations that are performed across the entirety of the lowering process. The lowest level of translation from x86 to LLVM IR is accomplished via debugging information emitted by the LLVM JIT when the native code object is originally produced. A brief description of the implementation required to populate each level of the tagging dictionary is provided below.
\begin{itemize}
    \item \textbf{\tpl AST Node to SQL Operator}: We populate this level of the tagging dictionary during the first phase of code generation. We modify the code generator’s operator translators to emit tags whenever a new AST node is constructed.
    \item \textbf{\tpl Bytecode to \tpl AST Node}: We populate this level of the tagging dictionary during the second phase of code generation. We add functionality to the TPL compiler that allows us to inject a callback function that emits a tag whenever the bytecode generator emits a new bytecode instruction.
    \item \textbf{LLVM IR Instruction to \tpl Bytecode}: We populate this level of the tagging dictionary during query compilation. We manually insert calls to emit tags whenever a new IR instruction is inserted into the compiled module.
\end{itemize}

\subsubsection{Profile Sample Collection}

We utilize the Linux perf API to collect an execution profile for the JIT-compiled query as it is executed. We utilize event-based sampling on instruction retirement (\lstinline{INST_RETIRED.PREC_DIST}) events. A sample is collected every 5,000 events. To ensure the accuracy of the collected samples, we request precise instruction pointers (0 skid) in our samples; this enables the Processor Event Based Sampling (PEBS) feature on Intel platforms \cite{intelmanual}. This sampling configuration is the same as that utilized by Beischl et al. in their proof-of-concept \cite{beischl21}.

Samples are written to a memory-mapped ring buffer by the kernel at the specified interval. We read the raw samples from this ring buffer and serialize them to a JSON file for subsequent processing.

\subsubsection{Tailored Profiling}

We combine the information collected in the tagging dictionary with the samples collected during query execution to generate profiles for any intermediate representation within NoisePage. For example, tailored profiling enables implementation of the SQL \dbCode{EXPLAIN ANALYZE} command by mapping native machine instructions all the way back up to SQL operators. However, it might also be useful to analyze profiles for lower levels of abstraction such as \tpl functions. Our implementation of tailored profiling is flexible enough to support arbitrary post-processing of this kind.

\subsection{Programmatic Pass Application}

We are unable to use LLVM's new non-legacy function pass manager for some annoying build-related reasons. By simply assigning each pass an ID, (in fact just the index of the pass in the overall array of possible transforms), we are able to naively separate the design of our automated pass exploration into two components:

\subsubsection{Component \#1: Algorithms on Lists of Natural Numbers}

A set of transformation passes is conceptually represented as a list of natural numbers, possibly with repeats. Each number in the list maps to a specified LLVM transform pass. For example, [3,10,2,10] may correspond to the application of the LLVM transform passses [\textit{gvn}, \textit{simplifycfg}, \textit{adce}, \textit{simplifycfg}]. This representation makes enumerating the search space convenient:

\begin{lstlisting}
# Pick up to max_passes elements from passes.
def uniform_selection(passes, max_passes):
return [random(0, len(passes)) for _ in range(0, max_passes)]

# Mutate a random pass in the list.
def mutate(passes):
    passes[random(0, len(passes))] = random(0, len(passes))

# Either mutate, add, or delete a pass in the list. This is a modification of a genetic/evolutionary algorithm.
def genetic(passes):
    choice = random(1, 4)
    if x==1: passes[random(0,len(passes))]=random(0, len(passes))
    else if x == 2: passes.append(random(0, max_passes))
    else if x == 3: passes.erase(random(0, len(passes))
    # x == 4 is no-op
    
# Pick a random ordering of distinct passes such that each pass improves the time frame significantly.
def distinct_ordering(passes):
    passes.append(random(0,max_passes) s.t. passes are distinct)
\end{lstlisting}

Any typical search algorithms such as beam search, genetic evolution, etc. would be implementable in this framework. However, due to time constraints, our results (below) were largely obtained with simple genetic search.

\subsubsection{Component \#2: A Framework for Applying an Algorithm to Select Passes}

A typical code generation DBMS engine requires some finagling to get the execution time results reported back into the compilation pipeline. The issue is that compile-and-optimize level statistics will happen at a different time and call site than execution-time statistics. The combination of the above profiling subsystem and our framework for pass application is ongoing and is likely to extend beyond this course; the current state of the framework is only able to gather execution-time statistics for functions which are manually invoked. To better understand the last limitation, consider the following example of a typical \tpl plan for inserting into a table, which may have many functions:

\begin{lstlisting}
fun Setup() { ... }
fun Body() { 
    ...;
    insertIntoTable(&tableIterator, &row);
    ...;
}
fun Teardown() { ... }
\end{lstlisting}

Where the compiled \tpl program is invoked by explicitly executing \dbCode{Setup()}, \dbCode{Body()}, and \dbCode{Teardown()} in that order (modulo aborts or other exceptions). We can gather execution times for explicitly executed functions like \dbCode{Setup()}, \dbCode{Body()}, and \dbCode{Teardown()}, but we cannot gather times for the \dbCode{insertIntoTable()} function. Because the majority of our heuristics are currently focused on execution time, this is a significant and implementation-specific handicap to our approach, especially when we see a surprising amount of gain from optimizing these library functions instead of what is considered our "main" \tpl program. Please see a discussion of the results below.

\subsubsection{Discussion}

We would like to highlight the flexibility of this two-component approach in allowing feature-engineered passes to be treated the same way as regular LLVM passes, with an example shown below.

\begin{lstlisting}
{"pmenon",
[](llvm::legacy::FunctionPassManager &fpm) {
    // Add custom passes. Hand-selected based on empirical evaluation.
    fpm.add(llvm::createInstructionCombiningPass());
    fpm.add(llvm::createReassociatePass());
    fpm.add(llvm::createGVNPass());
    fpm.add(llvm::createCFGSimplificationPass());
    fpm.add(llvm::createAggressiveDCEPass());
    fpm.add(llvm::createCFGSimplificationPass());
}}, 
\end{lstlisting}

Additionally, in the DBMS context, care has to be taken to perform all profiling work in a "fake transaction", which is our phrase for a transaction that always gets aborted. This proved to be slightly tricky due to certain implementation details of how results were transmitted back from our JIT'ed code back into the network layer -- the code would normally write results back to the Postgres-compatible packet writer as they became available, for example. In this case, we benefit significantly from the concept of aborts being routine in a DBMS setting because it allows us to execute code that has side effects while essentially ignoring the side effects. It would be interesting to study how side effects are handled in other systems which attempt profile-guided optimization.

To track whether an improvement was truly made, we always use the minimum -- noise is an issue, but noise cannot make slow code go faster, and over the many (hundreds/thousands) of iterations of our function pass applications, the minimum should win out. The algorithms that are used must ensure that mistakes can be recovered from, e.g., by allowing for arbitrary pass addition, mutation, or deletion.

%% ==================================================================
%% EXPERIMENTAL SETUP
%% ==================================================================

\section{Experimental Setup}

NoisePage is an end-to-end DBMS which you can connect to over SQL.

To enable profiling, run \dbCode{SET profile\_enable = true;} over SQL. This will cause queries to undergo a fixed number of profiling runs before finally executing. Unfortunately, please note the following caveats:
\begin{itemize}
    \item The number of profile runs and customization of profile strategies is just compiled in right now for rapid iteration, though there is no reason that can't be a setting.
    \item The profiling subsystem is not yet ready to be hooked up, so some profile information is missing. However, all profile information that is present (execution times, optimize times, etc) and gathered is real.
\end{itemize}

Specifically, we use the following procedure to perform our evaluations:

\begin{itemize}
    \item (Optional) For a more detailed trace of what the algorithms are doing, look for and add/remove the defines \lstinline{#define NOPRINT} and \lstinline{#define NOPRINT2} in \textit{llvm\_optimizer.cpp} and \textit{executable\_query.cpp} respectively.
    \item Perform any schema setup, data loading, etc.
    \begin{itemize}
        \item Trivial example: \dbCode{CREATE TABLE foo (a int);}
        \item Involved example: Load TPC-C \cite{tpcc}, for example by using OLTPBenchmark \cite{oltpbench}
    \end{itemize}
    \item Enable profiling.
    \begin{itemize}
        \item Run \dbCode{SET profile\_enable = true;}
    \end{itemize}
    \item (Optional) Switch the baseline evaluation from \dbCode{NOOP} (running without optimizations) to \dbCode{PMENON} (expert configuration).
    \begin{itemize}
        \item Run \dbCode{SET pmenon\_enable = true;}
    \end{itemize}
    \item Execute any SQL queries. The profiling process prints to standard output.
    \begin{itemize}
        \item Unfortunately, due to time constraints, there are no visualizers. You can CTRL-F for \lstinline{PASS MARKER} to quickly jump to the baseline and/or final result.
        \item The profiling data resets between SQL queries.
    \end{itemize}
\end{itemize}

%% ==================================================================
%% EXPERIMENTAL EVALUATION
%% ==================================================================

\section{Experimental Evaluation}

%% ==================================================================
%% DISCUSSION, SURPRISES, AND LESSONS LEARNED
%% ==================================================================

\section{Discussion, Surprises, and Lessons Learned}

Discussion To summarize the 4 discussion points above (2 x \dbCode{INSERT}, 2 x \dbCode{SELECT}), \textbf{there was no observed appreciable benefit to applying transform passes} for OLTP workloads. Other tests that were more OLAP in nature such as \dbCode{SELECT avg(i\_price) FROM item}, which scans across 100k entries, also showed negligible benefit. The expert passes were quite consistent in reducing instruction count, but it did not translate to any kind of noticeable speedup. There are two possible takeaways, which is that for typical SQL queries:
\begin{itemize}
    \item The LLVM transform passes are mostly not profitable, so don't bother with any of them.
    \item The LLVM transform passes are mostly not profitable, but are very cheap to run (twice as long optimizing is still fractions upon fractions of a second), so you might as well do them just in case.
\end{itemize}

A different interpretation of the results may be that the NoisePage DBMS does not expose sufficient information to the compiler to optimize its workloads. We expect that the hand-picked passes that the expert used were indeed very effective for their use-case, where data was represented as essentially raw arrays in memory and were manipulated more directly. However, in integrating their work to our NoisePage DBMS, every tuple of data is now hidden behind a function call which performs the necessary concurrency control checks, and therefore it may have been an abstraction through which the LLVM transform passes were not able to optimize.

Additionally, we found that most of the time spent optimizing a \tpl program was not in the directly source-visible functions but rather in all of the supporting functions that are builtin to the \tpl language. It is not clear whether optimizations are one-size fits all for all kinds of workloads, but pre-compiling these into a separate library and finding a way to link them is another possible source of optimization time savings.

One lesson that we learned from the experience in general is the distinction between a high-level design description of a solution and the development effort required to implement the solution. Near the midpoint of the project, the TUM group published the paper describing the tailored profiling technique and its applications to profile collection across abstraction levels. With the design description provided in this paper, we assumed that completing our own implementation of the technique in our profiling subsystem would prove relatively simple. This assumption proved to be incorrect. Although the core idea underlying tailored profiling, the tagging dictionary, is a simple concept, we found that populating the dictionary requires significant additions to the existing code generation and compilation infrastructure in NoisePage. The difference between the level of implementation difficulty reported in the original paper \cite{beischl21} and our own experience is the result of differences in execution engine architectures. While both Umbra \cite{umbra} and NoisePage implement data-centric code-generation engines, Umbra’s design is more amenable to the addition of support for tag collection.

%% ==================================================================
%% CONCLUSION AND FUTURE WORK
%% ==================================================================

\section{Conclusion and Future Work}

In this section we present our conclusions and some potential directions for future work.

\subsection{Conclusions}

We identify the following technical conclusions from this project:
\begin{itemize}
    \item We now know how to implement a profiling subsystem via tailored profiling in NoisePage. More generally, we better understand the nuances involved in lowering and preserving useful abstractions in systems with multiple levels of domain-specific intermediate representations.
    \item We began with the thesis of, "it may be profitable to apply different LLVM transformation passes (to different functions in the same program) in a profile-guided way to hyper-optimize a specific program".
    \item We conclude that, at least in the NoisePage DBMS as it integrates \tpl today, you might as well not apply any optimization passes at all. You'll halve the time spent optimizing (microseconds range) and have essentially the same execution time.
\end{itemize}

\subsection{Future Work}

We identify the following potential directions for future work:
\begin{itemize}
    \item Currently, our profiling subsystem uses one of the readily available kernel time sources (Linux's \lstinline{CLOCK_MONOTONIC_RAW}) to record timestamps for profile samples. In their description of tailored profiling, Beishcl et al. claim that none of the time sources exposed by the Linux kernel provide sample timestamps that are sufficiently accurate \cite{beischl21}. To address this issue, the authors apply a patch to the Linux kernel to expose the processor's timestamp counter register (the value read by \lstinline{RDTSC} on x86) value in the profile samples written by perf. They subsequently map this timestamp counter to a wall-clock time with the help of a kernel module \cite{beischl21}. Future work on our profiling subsystem might include a more robust evaluation of the distribution of profile timestamps to determine if a similar approach might be necessary in our system. 
    \item The current implementation of our profiling subsystem adopts a simple configuration in which we sample hardware instruction retirement events. However, now that we have the infrastructure in place, extending the system to sample different events and record additional data with each sample is straightforward. This presents significant opportunities for future work. For instance, Beischel et al. generate profiles for memory access patterns by sampling on memory instruction retirement events (\lstinline{MEM_INST_RETIRED.ALL_LOADS}) \cite{beischl21}. Cache performance analysis should also be possible by sampling on the available cache reference (\lstinline{PERF_COUNT_HW_CACHE_REFERENCES}) and cache miss (\lstinline{PERF_COUNT_HW_CACHE_MISSES}) events.
    \item The current profiling framework for determining the optimal set of LLVM passes for a particular query has the ability to support many different kinds of search algorithms. Given a heuristic function that includes a combination of different profiling data points, such as memory references and execution times for different operators, it is possible to exploit the framework and implement many different search algorithms, then compare their results.
    \item The LLVM program that we currently optimize (which is generated in \tpl and lowered to LLVM IR) spends a significant proportion of its overall runtime in builtin library functions, such as \dbCode{tableInsert()}, \dbCode{projectedRowSetInt()}, \dbCode{joinHashTableInit()}, etc. Optimizing these builtin functions take a bulk of the optimization time. Therefore, it may be the case that we can pre-optimize just these functions in a one-size-fits-all manner and observe benefits similar to dynamic optimization without the associated runtime overhead.
\end{itemize}

%% ==================================================================
%% WORK DISTRIBUTION
%% ==================================================================

\section{Work Distribution}

All members of our group contributed equally to this project.

%% ==================================================================
%% BIBLIOGRAPHY
%% ==================================================================

% NOTE: Had to add this mysterious command to compile...
\newcommand{\newblock}{}

\newpage
\balance
\bibliographystyle{abbrv}
\bibliography{report}

\end{document}